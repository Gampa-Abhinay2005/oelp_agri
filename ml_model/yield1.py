# -*- coding: utf-8 -*-
"""yield.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OC82Lqftf8CIgBUHQUjWP9eJ7Y0ose2t
"""

import pandas as pd
import numpy as np
import matplotlib as mpl
import matplotlib.pyplot as mplpl
import seaborn as sb

"""### `Data Preprocessing`"""

df = pd.read_csv("ml_model/India Agriculture Crop Production.csv")
df.head(5)

df.info()

print(df['Year'].value_counts())

df['Year'] = df['Year'].str[:4].astype(int)

df['Year'].unique()

df['Year'].value_counts()

# Removing rows where year is 1997 and 2020 as data seems to be incomplete for the following year.
df = df.drop(df[(df['Year'] == '1997') | (df['Year'] == '2020')].index)
print(f"The number of rows remaining are {df.shape[0]}.")

## Checking for null values in the dataset
df.isnull().sum()

#removing null values from the dataset
df=df.dropna()
print('checking for null values after removing null values')
print(df.isnull().sum())
print(f"The number of rows remaining are {df.shape[0]}.")

print(df['Production Units'].unique())
print(df['Area Units'].unique())

"""Here all the area units in `hectare` but the production units are not same (`Tonnes , Nuts , Bales`) so we need to convert them into same units i.e i am converting them into `tonnes`

Considering 1 Kg = 10 coconuts then 1 Tonne = 10,000 Cocunts or 1 Nut = 0.0001 Tonne

1 Bale = 0.22 Tonne or 1 Tonne = 4.6 Bales
"""

# Converting Production Units to Tonnes
df['Production_in_tonnes'] = df.apply(
    lambda row: row['Production'] * 0.0001 if row['Production Units'] == 'Nuts' else
                row['Production'] * 0.22 if row['Production Units'] == 'Bales' else
                row['Production'],
    axis=1
)

df.head()

print(f'number of unique crops in the dataset: {df["Crop"].nunique()}')
print(f'number of unique states in the dataset: {df["State"].nunique()}')
print(f'number of unique districts in the dataset: {df["District"].nunique()}')
print(f'number of unique seasons in the dataset: {df["Season"].nunique()}')

df["Season"].unique()

#we changed the the untins so we need to also need to chane the yeild
df = df.drop('Yield',axis = 1)
df['Yield'] = df['Production_in_tonnes'] / df['Area']

df.head(5)

new_df = df.drop(['Production Units', 'Area Units','Production'], axis=1)

new_df.head()

from sklearn.preprocessing import OneHotEncoder

categorical_cols = ['State', 'District', 'Crop', 'Season']
df_encoded = pd.get_dummies(new_df, columns=categorical_cols, drop_first=True)

new_df.head()

from sklearn.preprocessing import MinMaxScaler

# Initialize the scaler
scaler = MinMaxScaler()

# Normalize the 'Area' column
new_df['Area'] = scaler.fit_transform(new_df[['Area']])

# Display the first few rows to verify
new_df.head()

# Normalize the 'Production_in_tonnes' column
new_df['Production_in_tonnes'] = scaler.fit_transform(new_df[['Production_in_tonnes']])

# Display the first few rows to verify
new_df.head()

new_df['Yield']= new_df["Yield"].round(2)

# Combine data by grouping on Crop and other relevant columns, excluding Year
combined_df = new_df.groupby(['Crop', 'Season', 'State', 'District'], as_index=False).agg({
    'Area': 'sum',
    'Production_in_tonnes': 'sum',
    'Yield': 'mean'
})

combined_df.head()

# 1. Check Correlation Matrix

correlation_matrix = combined_df.select_dtypes(include=['number','float']).corr()
print(correlation_matrix)
sb.heatmap(correlation_matrix, annot=True, cmap='coolwarm')

from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
# 2. Split the Data into Features and Target Variable
X = combined_df.drop(['Yield','Production_in_tonnes'], axis=1)  # Features
y = combined_df['Yield']  # Target variable

# Encode categorical columns
label_encoder_state = LabelEncoder()
label_encoder_district = LabelEncoder()

X['State'] = label_encoder_state.fit_transform(X['State'])
X['District'] = label_encoder_district.fit_transform(X['District'])
X['Crop'] = label_encoder_state.fit_transform(X['Crop'])
X['Season'] = label_encoder_state.fit_transform(X['Season'])

# Re-split the data after encoding
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

X_train.head()

from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error
from sklearn.metrics import r2_score



# 4. Initialize the Random Forest Regressor
rf_model = RandomForestRegressor(n_estimators=100, random_state=42)

# 5. Fit the Model to the Training Data
rf_model.fit(X_train, y_train)

# 6. Make Predictions on the Test Data
y_pred = rf_model.predict(X_test)

# 7. Evaluate the Model
r2 = r2_score(y_test, y_pred)
print(f"R^2 Score: {r2}")
accuracy = rf_model.score(X_test, y_test)
print(f"Accuracy: {accuracy}")
mse = mean_squared_error(y_test, y_pred)
print(f"Mean Squared Error: {mse}")
import matplotlib.pyplot as plt
importances = rf_model.feature_importances_
feature_names = X.columns
indices = np.argsort(importances)[::-1]

# Print the feature ranking
print("Feature ranking:")
for f in range(X.shape[1]):
    print(f"{f + 1}. {feature_names[indices[f]]} ({importances[indices[f]]})")
# Plot the feature importances of the forest
plt.figure(figsize=(12, 6))
plt.title("Feature importances")
plt.bar(range(X.shape[1]), importances[indices], align="center")
plt.xticks(range(X.shape[1]), feature_names[indices], rotation=90)
plt.xlim([-1, X.shape[1]])
plt.show()
# 8. Save the Model
import joblib
joblib.dump(rf_model, 'random_forest_model.pkl')

accuracy_percentage = r2 * 100
print(f"Accuracy: {accuracy_percentage:.2f}%")

from sklearn.model_selection import RandomizedSearchCV

param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [ 2, 4]
}

random_search = RandomizedSearchCV(estimator=RandomForestRegressor(),
                                   param_distributions=param_grid,
                                   cv=5, n_iter=20, scoring='r2', n_jobs=-1)
random_search.fit(X_train, y_train)
print("Best RÂ² Score:", random_search.best_score_)
best_model = random_search.best_estimator_

best_model.get_params()

accuracy = best_model.score(X_test, y_test)
print(f"Accuracy: {accuracy}")

joblib.dump(best_model, 'random_forest_best_model.pkl')

new_df['District'].unique()
new_df['State'].unique()

def predict_yield(crop, state, district, area, season):
    # Load the model
    model = joblib.load('random_forest_best_model.pkl')

    # Encode the input data
    # Check if the input labels exist in the encoder's classes
    if state not in label_encoder_state.classes_:
        label_encoder_state.classes_ = np.append(label_encoder_state.classes_, state)
    if district not in label_encoder_district.classes_:
        label_encoder_district.classes_ = np.append(label_encoder_district.classes_, district)
    if crop not in label_encoder_state.classes_:
        label_encoder_state.classes_ = np.append(label_encoder_state.classes_, crop)
    if season not in label_encoder_state.classes_:
        label_encoder_state.classes_ = np.append(label_encoder_state.classes_, season)

    # Transform the input labels
    state_encoded = label_encoder_state.transform([state])[0]
    district_encoded = label_encoder_district.transform([district])[0]
    crop_encoded = label_encoder_state.transform([crop])[0]
    season_encoded = label_encoder_state.transform([season])[0]

    # Create a DataFrame for the input data
    input_data = pd.DataFrame({
        'Crop': [crop_encoded],
        'Season': [season_encoded],
        'State': [state_encoded],
        'District': [district_encoded],
        'Area': [area]
    })

    # Reorder columns to match the training data
    input_data = input_data[X.columns]

    # Make prediction
    predicted_yield = model.predict(input_data)
    return predicted_yield[0]
# Example usage
predicted_yield = predict_yield(crop='Rice', state='Andhra Pradesh', district='KRISHNA', area=1000, season='Kharif')
print(f"Predicted Yield: {predicted_yield:.2f} tonnes per hectare")

from xgboost import XGBRegressor
model = XGBRegressor()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
r2 = r2_score(y_test, y_pred)
print(f"R^2 Score: {r2}")

accuracy = model.score(X_test, y_test)
print(f"Accuracy: {accuracy}")

